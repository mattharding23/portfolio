---
title: "EOTB Data Cleaning"
output: html_notebook
---
## Libraries
```{r}
library(tidyverse)
library(lubridate)
library(GGally)


```
## Import Data
```{r}
# Data files were downloaded as seperate csv files based on location. First import the data, then view it, then try to combine files. 
# Data spans from Jan 1, 2007 to Jan 1, 2021. Dates and locations were chosen based on available data. 

# Import the csv
data_ironpot = read.csv('EOTB_Iron_Pot_Landing.csv',na.string = c("", " "))
# View the data
#head(data_ironpot)

data_jugbay = read.csv('EOTB_Jug_Bay.csv',na.string = c("", " "))
#head(data_jugbay)

data_mataponi = read.csv('EOTB_Mataponi.csv',na.string = c("", " "))
#head(data_mataponi)

data_newport = read.csv('EOTB_Newport_Creek.csv',na.string = c("", " "))
#head(data_newport)

data_public = read.csv('EOTB_Public_Landing.csv',na.string = c("", " "))
#head(data_public)

data_sandy = read.csv('EOTB_Sandy_Point.csv',na.string = c("", " "))
#head(data_sandy)

data_stgeorge = read.csv('EOTB_St_Georges.csv',na.string = c("", " "))
#head(data_stgeorge)

```
## Combine Data into one data frame
```{r}
dataset = rbind(data_ironpot,data_jugbay,data_mataponi,data_newport,data_public,data_sandy,data_stgeorge)
ind = 1:nrow(dataset)
dataset = data.frame(ind,dataset)

```

## Shorten Station Names for Ease of identifying location 
```{r}
# View if there are any NA values in station column & Make sure there are no mistakes in identifying station. 
sum(is.na(dataset$Station))
table(dataset$Station)
# There are no NA values & the table shows 7 values so I will move forward with remnaming the stations 
dataset$Station[dataset$Station == ' Patuxent River - Iron Pot Landing'] = 'IP'
dataset$Station[dataset$Station == ' Coastal Bays - Newport Creek'] = 'NC'
dataset$Station[dataset$Station == ' Coastal Bays - Public Landing'] = 'PL'
dataset$Station[dataset$Station == ' Patuxent River - Jug Bay'] = 'JB'
dataset$Station[dataset$Station == ' Patuxent River - Mataponi'] = 'MP'
dataset$Station[dataset$Station == ' Potomac River - St. Georges Creek'] = 'SG'
dataset$Station[dataset$Station == ' Chesapeake Bay Segment 3 - Sandy Point South Beach'] = 'SP'
# View names of stations after renaming to ensure there are no mistakes. 
table(dataset$Station)

```
```{r}
# Change datetime column to datetime class 
dataset$DateTime = lubridate::parse_date_time(dataset$DateTime, "mdy HM")

sapply(dataset,class)
```



```{r}
# From viewing data I immediately see that some columns appear to be completely blank but given the large amount of data rather than trying to look at the columns and delete the ones with no data, I am going to write a function that does it for me. 

#any(!is.na(CMC_data$Air.temperature..AT.1.))

blank_col_remove <- function(df){

  # names of removed columns to keep track
  rem_names = c()

  # amount of NA values in column if not removed
  vals = c()
  
  for(i in colnames(df)){
    dat_col = df[i]
    
    # if there are any no-NA values, sum the total number of NA values & store in vector
    if (any(!is.na(dat_col))){
      vals = append(vals,sum(is.na(dat_col)))
    }
    # If there are no non-NA values, delete that column from the dataframe and store column name in vector
    else{
      df = df[,!(names(df) %in% i)]
      rem_names = append(rem_names,i)
    }
  
  }
  info = list(df,vals,rem_names)
  return(info)
}

```

```{r}
# Remove completely blank columns
new_dat = blank_col_remove(dataset)

# Keep Track of removed columns
removed_columns = new_dat[[3]]

# Number of NA values associated with each column
value_counts = new_dat[[2]]

# Data with columns removed
dataset = new_dat[[1]]
```
## View NAs
```{r}
# View amount of NAs with column names
NA_count = data.frame(colnames(dataset),value_counts)
NA_count
# From looking at this I can see that I have a lot of missing data, I will have to go through each column to see what to do with the missing data
```

## Remove Date and time columns
```{r}
# There is a datetime column so date and time columns are redundent
dataset = dataset[,!(names(dataset) %in% c('Sample_date','Sample_Time'))]
removed_columns = append(removed_columns,c('Sample_date','Sample_Time'))
```

## Layer column cleaning
```{r}
# I want to know whether or not the "layer" column will be useful so I want to look at what values it has.
table(dataset$Layer)

#Every value in this column is the same so thats not very useful. I am going to remove this column. 
dataset = dataset[,!(names(dataset) %in% c('Layer'))]
rem_names = append(removed_columns,'Layer')
```
## Cleaning & Combining Temperature Columns
```{r}
# There are two temperature columns -> one being in F other being in C. I want to combine them to be one column. However there is missing data and I need to clean them up first. 

# The first thing I want to look at is if there are rows in which both columns have missing values. 
f = is.na(dataset$T_Fahr)
c = is.na(dataset$Temp_C)
comb = f & c
sum(comb)
# Unfortunately it appears that all of the rows are missing both columns. So now I want to just take a look at this subset of data
temp = dataset %>%
  filter(is.na(T_Fahr))
# After taking a look at it, it appears that all the data is missing in most of these rows but I am going to confirm that. 
table(temp$Station)
# There is missing data at all stations. 
not_na_temp = c()
for (i in colnames(temp)){
  count = sum(!is.na(temp[i]))
  not_na_temp = append(not_na_temp, count)
}
not_na_temp
# So out of 93987 values, only a little over 5000 rows have any quantititative data in them. I don't have a choice but to delete these rows. 
```
```{r}
# I don't want to delete the rows that contain data yet though so I am going to get rid of the rows with no data in them first
inds = c()
# store all rows indicies in the df that are not NA
for (i in colnames(dataset[4:12])){
  ind = which(!is.na(dataset[i]))
  inds = append(inds,ind)
  
}
# Get only unique indicies
inds = unique(inds)
# Subset the data to exclude those rows. 
dataset = dataset[inds,]

# Check how many missing rows Temperature is missing now. 
sum(is.na(dataset$T_Fahr))

temp = dataset %>%
  filter(is.na(T_Fahr))

## Looking again at this subset of missing temperature data, there is only a small set of rows that contain any quantitiative data beyond sample depth. This is not useful so I will now delete those rows using the same process as before. 

```

```{r}
inds = c()
# store all rows indicies in the df that are not NA
for (i in colnames(dataset[5:12])){
  ind = which(!is.na(dataset[i]))
  inds = append(inds,ind)
  
}
# Get only unique indicies
inds = unique(inds)
# Subset the data to exclude those rows. 
dataset = dataset[inds,]

# Look at missing Temp data subset again
temp = dataset %>%
  filter(is.na(T_Fahr))
# There are a few other rows (10 or so) that only have one or two other pieces of quantitative information and are missing the most important pieces of information So I am choosing to get rid of those as well 

# I chose to delete on pH because that is an important data column and also has an NA value in all of the rows I am deleting. 

dataset = dataset[!(1:nrow(dataset)) %in% which((is.na(dataset$T_Fahr))&(is.na(dataset$pH))),]
# And again, look at temp data subset
temp = dataset %>%
  filter(is.na(T_Fahr))
# Looks pretty good. Now I can find a way to replace temperature data. I will look for correlations between temperature and other quantitative columns
#I need to subset the data so I will just choose 100000 data points to look at
#ggpairs(dataset[,c(4:11)])
#ggsave('temp_corr_mat.png')
```

```{r}
# It looks like there is some correlation with dissolved oxygen so I want to view that on a larger plot. 

# ggplot(data = dataset )+
#   geom_point(aes(T_Fahr,DO_mg.L)) +
#     labs(
#     x = 'Temperature (F)',
#     y = 'Dissolved Oxygen (mg/L)',
#     subtitle = 'Temperature vs Dissolved Oxygen',
#     title = 'Correlation Plot'
#   )+
#   theme(plot.title = element_text(size=17, face="bold", 
#     margin = margin(10, 0, 10, 0),hjust =.5),
#     plot.subtitle = element_text(hjust =.5))
# ggsave('TvsDO.png')
```

```{r}

# Seeing this on a larger plot, there is no correlation. 

# Finding no correlation with temperature but understanding temperature, I should be able to replace temperature values with the average temperature for that month. 

# Make new column that identifies the month the sample was taken in. 
month_col = month(dataset$DateTime)
dataset = data.frame(dataset,month_col)

# Get mean temperatures for each month
mon_temps = c()
for( i in 1:12){
  dat = dataset[dataset$month_col == i,]
  x = mean(dat$T_Fahr, na.rm =T)
  mon_temps = append(mon_temps,x)
}
# Put mean temps in a dataframe with month
mon = 1:12
mean_temps = data.frame(mon, mon_temps)

# Place mean temps into temporary dataframe with only rows containing NA values in temp column
temp = dataset %>%
  filter(is.na(T_Fahr)) %>%
  left_join(mean_temps, by = c('month_col' = 'mon'))
  
temp$T_Fahr = temp$mon_temps
temp = temp[,!(names(temp) %in% 'mon_temps')]
# Delete rows with NA values in Temp column
dataset = subset(dataset,!is.na(dataset$T_Fahr))
# Replace those rows with Temp dataframe
dataset = rbind(dataset,temp)
# Delete Celcius column

dataset = dataset[,!(names(dataset) %in% 'Temp_C')]
rem_names = append(rem_names,'Temp_C')
```

# View NAs again
```{r}
apply(dataset, 2, function(x) sum(is.na(x)))
```

## Sample Depth Cleaning
```{r}
# I want to look at a summary of the values of the sample depth data
samd = dataset$SampleDepth_m 
summary(samd)
# That is a rather large range for the depth the sample was taken at

# Now I want to know which stations the missing data is associated with
table(dataset$Station[is.na(samd)])
# Every station has missing data here so that is not very helpful.

# So I will see if sample depth has a strong correlation with any other variables. 

#for (i in colnames(dataset[7:13])){
#  x = dataset$SampleDepth_m[!is.na(dataset$SampleDepth_m) & !is.na(dataset[i])]
#  y = dataset[i][!is.na(dataset$SampleDepth_m) & !is.na(dataset[i])]
#  plot(x,y)
#}

p = c()
for (i in colnames(dataset[5:11])){
  x = dataset$SampleDepth_m[!is.na(dataset$SampleDepth_m) & !is.na(dataset[i])]
  y = dataset[i][!is.na(dataset$SampleDepth_m) & !is.na(dataset[i])]
  p = append(p,cor(x,y, method = 'spearman'))
}
p

## Given that sample depth does not appear to correlate with anything else and is not a very informative data point I am going to drop this column of data 
```

```{r}
dataset = dataset[,!(names(dataset) %in% 'SampleDepth_m')]
rem_names = append(rem_names,'SampleDepth_m')

```

```{r} 
## On the topic of removing columns I am not interested in, There are two dissolved oxygen columns, I am going to keep the column that is in terms of percentage and delete the other one.
dataset = dataset[,!(names(dataset) %in% 'DO_mg.L')]
rem_names = append(rem_names,'DO_mg.L')
```

## Next I will look at salinity
```{r}

sal = dataset$Salinity_ppt
summary(sal)
sd(sal,na.rm = T)
# Looking at distribuion, it has a large standard deviation so probably can't use the mean for NAs. 
# So I will take a look at the data that only contains NAs now

salna  = dataset %>%
  filter(is.na(Salinity_ppt))

# Now look for correlations. 
p = c()
for (i in colnames(dataset[5:10])){
  x = dataset$Salinity_ppt[!is.na(dataset$Salinity_ppt) & !is.na(dataset[i])]
  y = dataset[i][!is.na(dataset$Salinity_ppt) & !is.na(dataset[i])]
  p = append(p,cor(x,y, method = 'spearman'))
}
p
```
```{r}
# I can see that there is a pretty strong correlation between salinity and ph
sum(is.na(salna$pH))
# Unfortunately a good amount of the ph data is missing in this subset. So I want to look a that subset now.
sal_ph_na = dataset %>%
  filter(is.na(Salinity_ppt)&is.na(pH))
# Just those two column of missing data. So now I want to see if maybe its cyclical like temperature.
```
```{r}

#ggplot()+
#  geom_line(aes(dataset$Salinity_ppt[!is.na(dataset$Salinity_ppt)&dataset$Station == 'IP'],1:length(dataset$Salinity_ppt[!is.na(dataset$Salinity_ppt)&dataset$Station == 'IP'])))


```

```{r}
mon_sal = c()
for( i in 1:12){
  dat = dataset[dataset$month_col == i,]
  x = mean(dat$Salinity_ppt[dat$Station == 'IP'], na.rm =T)
  # Uncomment to see standard deviation next to mean
  #y = sd(dat$Salinity_ppt[dat$Station == 'IP'], na.rm =T) 
  mon_sal = append(mon_sal,x)
}
mon_sal
for (i in unique(dataset$Station)){
  for(j in 1:12){
    new_sal = mean(dataset$Salinity_ppt[dataset$month_col == j & dataset$Station == i],na.rm = T)
    dataset$Salinity_ppt[(dataset$Station == i) & (dataset$month_col == j) & (is.na(dataset$Salinity_ppt)) ] = new_sal

  }
}
# The standard deviation of the salinity at a single station by month is actually very small. So I will replace NA values with the mean at each station in each month

sum(is.na(dataset$Salinity_ppt))
```
```{r} 
# Since there is a strong correlation between salinity and ph I can now replace NA values in ph using a linear regression that also accounts for month of the year and the station the data was gathered at. 

reg = lm(pH ~ Salinity_ppt+ month_col+Station, dataset, na.action = na.omit)
summary(reg)
y1 = reg$coefficients[1]
y2 = reg$coefficients[2]
dataset$pH[is.na(dataset$pH)] = y2*(dataset$Salinity_ppt[is.na(dataset$pH)])+y1
sum(is.na(dataset$pH))

```

```{r}
#ggpairs(dataset[,c(6:12)])
#ggsave('pairs.png')
```
```{r} 
# Nothing else really correlates so I will either have to delete missing data or find a way inside that column to replace the data. 
dis_ox_na = dataset[is.na(dataset$DO_pctSat),]

mon_do = c()
for( i in 1:12){
  dat = dataset[dataset$month_col == i,]
  x = mean(dat$DO_pctSat[dat$Station == 'JB'], na.rm =T)
  y = sd(dat$DO_pctSat[dat$Station == 'JB'], na.rm =T)
  mon_do = append(mon_do,c(x,y))
}
# The standard deviations of Disolved oxygen compared to the mean are very large even when filtered by month and collecting station. I will have to delete these rows of NA values. 
dataset = dataset[!is.na(dataset$DO_pctSat),]


```

# View NAs again
```{r}
apply(dataset, 2, function(x) sum(is.na(x)))

# There are two columns left with NAs
```

## Turbidity
```{r}
turb_na = dataset[is.na(dataset$Turb_NTU),]

mon_turb = c()
for( i in 1:12){
  dat = dataset[dataset$month_col == i,]
  x = mean(dat$Turb_NTU[dat$Station == 'JB'], na.rm =T)
  y = sd(dat$Turb_NTU[dat$Station == 'JB'], na.rm =T)
  mon_turb = append(mon_turb,c(x,y))
}
mon_turb

# The standard deviations of turbidity compared to the mean are very large even when filtered by month and collecting station. I will have to delete these rows of NA values. 
dataset = dataset[!is.na(dataset$Turb_NTU),]
```

# View NAs again
```{r}
apply(dataset, 2, function(x) sum(is.na(x)))

# just one column left with NAs
```

```{r}
chl_na = dataset[is.na(dataset$Chl_ug.L),]

mon_chl = c()
for( i in 1:12){
  dat = dataset[dataset$month_col == i,]
  x = mean(dat$Chl_ug.L, na.rm =T)
  y = sd(dat$Chl_ug.L, na.rm =T)
  mon_chl = append(mon_chl,c(x,y))
}
mon_chl

# Once again, the standard deviations of chloroplasm compared to the mean are very large even when filtered by month and collecting station. I will have to delete these rows of NA values. 

dataset = dataset[!is.na(dataset$Chl_ug.L),]
```
```{r}
# I have gotten rid of missing values, all columns are the correct class. Now I need to make sure there is not any incorrect or wrong data. 
sums = apply(dataset[,4:9], 2, function(x) summary(x))
```

```{r}
# The salinity data has a large difference between mean and median so I want to view it as a histogram
ggplot() +
  geom_histogram(aes(x = dataset$Salinity_ppt),bins = 10)+
  labs(
    x = 'Salinity (ppt)',
    y = 'Count',
    title = 'Histogram of Salinity Values'
  )+
  theme(plot.title = element_text(size=17, face="bold",
    margin = margin(10, 0, 10, 0),hjust =.5))
ggsave('salinity.png')
# It is definitely skewed, however googling typical salinity values in brackish water, all of these values are within the correct range so I will not change them. 


```

```{r}
# pH looks good
# At First I was concerned about oxygen saturation percentages because there are large values over 100%. However, after some research, it turns out oxygen saturation % can be that high
t = dataset[dataset$Turb_NTU <0 ,]
ggplot() +
  geom_histogram(aes(x = dataset$Turb_NTU), bins = 200)
# There are some turbidity values that seemed a bit high. I was not able to find any definitive ranges and given that it is a small amount of values compared to the whole dataset I am going to leave it in. However, there are a fair amount of negative values which are not possible and indicate user error so I need to do something about those. Unfortunately, I am going to have to delete these rows as there is not a good method to replace them. 
dataset = dataset[dataset$Turb_NTU > 0,]
```
```{r}
ggplot() +
  geom_histogram(aes(x = dataset$Turb_NTU), bins = 75)
```

```{r}
# Chlorphyll measurements also can't be negative. There are some negative values so I have to get rid of those too. 
t = dataset[dataset$Chl_ug.L <0 ,]
ggplot() +
  geom_histogram(aes(x = dataset$Chl_ug.L), bins = 50)

# Luckily there are only 6 rows of this.  
dataset = dataset[dataset$Chl_ug.L > 0,]
# There are some high numbers as well but it seems as though high values are typically correct and happen from time to time. 
```

```{r}
#Now with data sufficiently cleaned I want to explore it a little.
# The first thing I want to do is create a correlation matrix. 

#ggpairs(dataset[2:10])
#ggsave('corr_mat.png')


```
```{r}
# I notice the data seems to be skewed but I realized that its not ordered properly by data so I must do that ad then view it again
dataset = dataset[order(dataset$DateTime),]
#ggpairs(dataset[2:10])
#ggsave('corr_mat_order_by_date.png')


```

```{r}
write.csv(dataset,'EOTB_Cleaned.csv')

```
