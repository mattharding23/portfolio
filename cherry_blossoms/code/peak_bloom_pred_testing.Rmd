---
title: "Peak Bloom Prediction"
author: "Matt ardingr"
output:
  html_document:
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, error = FALSE,
  message = FALSE,
  fig.align = "center",
  out.width = "80%"
)
``` 

# Initial EDA

### Import Libraries
```{r}
library(tidyverse)
library(rnoaa)
library(leaps)
library(caret)
```

### Read in data
```{r}
cherry <- read_csv("data/washingtondc.csv") %>%
  bind_rows(read_csv("data/liestal.csv")) %>%
  bind_rows(read_csv("data/kyoto.csv"))
```

### EDA
```{r}
# View bloom day by year
ggplot(data = cherry[cherry$year > 1900, ]) +
  geom_point(aes(x = year, y = bloom_doy)) +
  facet_wrap(~location)

```

# Find external weather data

Meteostat has historical weather data by location. To extract the data I need, I found the closest weather station to each latitude and longitude and then was able to download a .csv file of that locations historical data. 

https://dev.meteostat.net/bulk/stations.html#endpoints
https://dev.meteostat.net/bulk/daily.html#endpoints


DC - https://bulk.meteostat.net/v2/daily/72405.csv.gz
Vancouver - https://bulk.meteostat.net/v2/daily/71892.csv.gz
Kyoto - https://bulk.meteostat.net/v2/daily/47759.csv.gz

There was not data available for Liestal so I will use the NOAA database for that. 

### Find Weather data 
```{r}
# Import json file
library(rjson)
json_data <- fromJSON(paste(readLines("data/full.json"), collapse = ""))
```

### Convert json to df to filter for regions needed
```{r}
# Set up data frame
df <- tibble(
  "id" = character(),
  "name" = character(),
  "country" = character(),
  "region" = character(),
  "national" = character(),
  "wmo" = character(),
  "icao" = character(),
  "lat" = character(),
  "long" = character(),
  "elev" = character(),
  "ind" = character()
)

# Loop through json and convert to df rows
for (i in 1:length(json_data)) {
  y <- json_data[i]
  x <- y[[1]]
  id <- x[[1]]
  name <- x[[2]][[1]]
  country <- x[[3]]
  region <- x[[4]]
  if (length(region) == 0) {
    region <- NA
  }
  national <- x[[5]][[1]]
  if (length(national) == 0) {
    national <- NA
  }
  wmo <- x[[5]][[2]]
  if (length(wmo) == 0) {
    wmo <- NA
  }
  icao <- x[[5]][[3]]
  if (length(icao) == 0) {
    icao <- NA
  }
  lat <- as.character(x[[6]][[1]])
  long <- as.character(x[[6]][[2]])
  elev <- as.character(x[[6]][[3]])
  ind <- as.character(i)
  loc_entry <- list(id, name, country, region, national, wmo, icao, lat, long, elev, ind)
  df[i, ] <- loc_entry
}
```

### Find closest station for each location
```{r}
df$lat <- as.numeric(df$lat)
df$long <- as.numeric(df$long)

US_df <- df %>%
  filter(country == "US" & region == "DC")

CA_df <- df %>%
  filter(country == "CA" & region == "BC" & lat > 49.1 & lat < 49.3 & long > -123.3 & long < -123 & national == "51442")

Ky_df <- df %>%
  filter(country == "JP" & lat > 34.9 & lat < 35.2 & long > 135.4 & long < 135.8)

Sw_df <- df %>%
  filter(lat > 47.4 & lat < 47.7 & long > 7.6 & long < 7.85)
```

# Use DC weather data to begin building a model
Starting with DC weather to build an initial model

### Import DC weather
```{r}
## Read in csv file
dc_weather <- read_csv("data/DC_weather.csv")

# set column names
weather_names <- c("date", "tavg_C", "tmin_C", "tmax_C", "precip_mm", "snow_mm", "wind_dir_deg", "wind_sp_km/h", "peak_wind_km/", "pres_hPa", "sun_min")
colnames(dc_weather) <- weather_names

# Separate month, day & year
dc_weather <- dc_weather %>%
  extract(date,
    into = c("year", "month", "day"),
    regex = "([0-9]+)[-]([0-9]+)[-]([0-9]+)$"
  ) %>%
  # Make individual date variables numeric
  mutate(across(c(year, day, month), as.numeric)) %>%
  # Filter for years that precipitation was included & no missing values of temperature which would include years after 1946.
  filter(year >= 1946) %>%
  # add total days count column by year
  group_by(year) %>%
  mutate("days_into_year" = row_number()) %>%
  # Replace missing tavg values with avg of min and max tempuratures of said day
  # This is imperfect as the values are slightly different than the actual average for the day
  # However, I believe this to be more useful than deleting the rows.
  ungroup() %>%
  group_by(year, day, month) %>%
  mutate("avg_T" = (tmax_C + tmin_C) / 2)
dc_weather$tavg_C[is.na(dc_weather$tavg_C)] <- dc_weather$avg_T[is.na(dc_weather$tavg_C)]

# Remove unneeded variables
# Note: Wanted to keep the sun minutes variable, however, there were many years of missing values
# after trying to build a model to replace values the model was not able to capture the noise in the sun minutes variable
# and therefore I felt it best to leave the variable out.
dc_weather <- dc_weather %>%
  select(-avg_T, -snow_mm, -wind_dir_deg, -`wind_sp_km/h`, -`peak_wind_km/`, -pres_hPa, -sun_min)

# convert celcius to kelvin - I plan to find cumulative values of temperature and having negative values was an issue.
# So I chose to use an absolute scale (Kelvin) as opposed to Celcius.
dc_weather$tavg_C <- dc_weather$tavg_C + 273.15
dc_weather$tmin_C <- dc_weather$tmin_C + 273.15
dc_weather$tmax_C <- dc_weather$tmax_C + 273.15

colnames(dc_weather)[4:6] <- c("tavg_K", "tmin_K", "tmax_K")
write_csv(dc_weather, "dc_weather_clean")
```

### Get DC blossom data
```{r}
# Read in DC cherry tree data
dc_cherry <- read_csv("data/washingtondc.csv")
```


### Create function to merge weather data with bloom data to build model
```{r}

bloom_merge <- function(bloom, weather, yr) {
  
#' Create and save variables to use in models for predicting cherry 
#'  blossom blooms
#'
#' Takes a data frame of bloom data and a data frame of weather data from same
#'   location and a year and calculates variables to use in model

  # Filter bloom data by year
  bloom <- bloom %>%
    filter(year == yr)

  # Filter weather data to get only data from that year and first 150 days
  weather <- weather %>%
    filter(year == yr & days_into_year <= 150)

  # Save day of year of bloom that year
  bdoy <- as.numeric(bloom[7])
  consec = 0
  count = 14
  bloom$warm_start = 150
  while(consec == 0 & count < 150){
    if(all(weather$tavg_K[(weather$days_into_year >= count-6) & (weather$days_into_year <= (count))] >= 280)){
      bloom$warm_start = count - 13
      consec = 1
    }
    count = count + 1
  }


  # Cumulative weather data from days 61-90 of the year
  bloom$third_30_avgT <- sum(weather$tavg_K[61:90])
  bloom$third_30_maxT <- sum(weather$tmax_K[61:90])
  bloom$third_30_minT <- sum(weather$tmin_K[61:90])
  bloom$week70_84_avg_t <- sum(weather$tavg_K[70:84])
  bloom$week70_84_max_t <- sum(weather$tmax_K[70:84])
  bloom$week70_84_min_t <- sum(weather$tmin_K[70:84])
  bloom$week85_98_avg_t <- sum(weather$tavg_K[85:98])
  bloom$week85_98_max_t <- sum(weather$tmax_K[85:98])
  bloom$week85_98_min_t <- sum(weather$tmin_K[85:98])

  return(bloom)
}
```

Note: I initially wanted to include precipitation in my model however it actually made my model less accurate so I dropped it from the model. 

## Create Dataset to model DC blooms

### Dataset for modeling DC
```{r}
# Filter cherry tree data for past 1946 (first point in which there is precipitation data)
dc_cherry_w_weath <- dc_cherry %>%
  filter(year >= 1946)

# Initialize tibble with first row
dc_bloom <- as_tibble(bloom_merge(dc_cherry_w_weath, dc_weather, 1946))

# Loop through all other rows to create data frame with all variables from bloom_merge function
for (i in 2:nrow(dc_cherry_w_weath)) {
  yr <- dc_cherry_w_weath$year[i]
  dc_bloom[i, ] <- bloom_merge(dc_cherry_w_weath, dc_weather, yr)
}
write_csv(dc_bloom, "dc_model_data.csv")
```

## Try a ridge regression model

### Ridge regression for bloom data
```{r}
set.seed(123)

# Create training and test sets
training_samples <- dc_bloom$bloom_doy %>%
  createDataPartition(p = 0.8, list = FALSE)
train_data <- dc_bloom[training_samples, c(2,7:16)]
test_data <- dc_bloom[-training_samples, c(2,7:16)]

# Set up data in matrix to run ridge regression
x <- model.matrix(bloom_doy ~ ., train_data)[, -1]
y <- train_data$bloom_doy

# Run cross validation to find best lambda value
cv_ridge <- cv.glmnet(x, y, alpha = 0)
cv_ridge$lambda.min

# Fit ridge regression model to training data using optimal lambda value found
# from cross validation
fit_ridge_dc <- glmnet(x, y, alpha = 0, lambda = cv_ridge$lambda.min)

# View coefficients
# coef(fit_ridge)

# Set up testing data to test predictability
x_test <- model.matrix(bloom_doy ~ ., test_data)[, -1]

# Make predictions from ridge regression model
ridge_pred_dc <- fit_ridge_dc %>%
  predict(x_test) %>%
  as.vector()

# View model performance metrics
tibble(
  RMSE = RMSE(ridge_pred_dc, test_data$bloom_doy),
  Rsquare = R2(ridge_pred_dc, test_data$bloom_doy)
)
```

## Try a lasso regression model next

### Run lasso Regression
```{r}
## Use lasso to see if model as fewer strong variables

# Run cross validation to find best lambda value
cv_lasso <- cv.glmnet(x, y, alpha = 1)

# Fit lasso regression model to training data using optimal lambda value found
# from cross validation
fit_lasso_dc <- glmnet(x, y, alpha = 1, lambda = cv_lasso$lambda.min)

# View coefficients
# coef(fit_lasso)

# Make predictions from lasso regression model
lasso_pred_dc <- fit_lasso_dc %>%
  predict(x_test) %>%
  as.vector()

# View model performance metrics
tibble(
  RMSE = RMSE(lasso_pred_dc, test_data$bloom_doy),
  Rsquare = R2(lasso_pred_dc, test_data$bloom_doy)
)
```

## Finally try elastic net regression

### Elastic Net regression
```{r}
## Using elastic net regression to try to find best combination of alpha and lambda parameters

# Build the model using the training set
set.seed(1212)
cv_elas_net <- train(
  bloom_doy ~ .,
  data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)

# Best tuning parameter
(en_lambda <- cv_elas_net$bestTune)
```


### Elastic Net Regression
```{r}
# Fit elastic net regression using alpha and lambda found from cross validation
fit_net_dc <- glmnet(x, y, 
                       alpha = cv_elas_net$bestTune[1], 
                       lambda = cv_elas_net$bestTune[2])

# View coefficients
coef(fit_net_dc)


# Make predictions from ridge regression model
net_pred_dc <- fit_net_dc %>%
  predict(x_test) %>%
  as.vector()

# View model performance metrics
tibble(
  RMSE = RMSE(net_pred_dc, test_data$bloom_doy),
  Rsquare = R2(net_pred_dc, test_data$bloom_doy)
)
```

# View predictions vs actual
```{r}

dc_preds <- tibble("year" = dc_bloom$year[-training_samples],
                   "actual_bloom_doy" = test_data$bloom_doy, 
                   "ridge_reg_doy" = ridge_pred_dc,
                   "lasso_reg_doy" = lasso_pred_dc,
                   "net_elast_reg_doy" = net_pred_dc)

```

I feel comfortable that my model elastic net ridge regression model is relatively accurate for DC. I am curious as to whether it will translate well to other cities. I will next test Kyoto. 

# Move onto updating model with Kyoto Data

### Import Kyoto weather
```{r}
## Read in csv file
ky_weather <- read_csv("data/kyoto_weather.csv")

# set column names
weather_names <- c("date", "tavg", "tmin", "tmax", "precip_mm", "snow_mm", "wind_dir_deg", "wind_sp_km/h", "peak_wind_km/", "pres_hPa", "sun_min")
colnames(ky_weather) <- weather_names

# Separate month, day & year
ky_weather <- ky_weather %>%
  extract(date,
    into = c("year", "month", "day"),
    regex = "([0-9]+)[-]([0-9]+)[-]([0-9]+)$"
  ) %>%
  
  # Make individual date variables numeric
  mutate(across(c(year, day, month), as.numeric)) %>%
  
  # Filter for years where there is enough weather data to work with
  filter(year >= 1946) %>%
  
  # add total days count column by year
  group_by(year) %>%
  mutate("days_into_year" = row_number()) %>%
  ungroup()
```

### Replace NA values for Tmax with a modeled version
```{r}
# Create dummy data frame for values that are not NA in the shown variables
ky_na <- ky_weather %>%
  filter(!is.na(tmax) | !is.na(tmin) | !is.na(tavg))

set.seed(1212)

# For 10- fold cross validation
k <- 10 

folds <- sample(1:k, nrow(ky_na), replace = TRUE)

# Place holder for errors
cv.errors <- c() 

# Write a for loop that performs cross-validation
for (i in 1:k) {
  dat <- ky_na[folds != i, ]
  cv_mod <- glm(tmax ~ tavg + days_into_year, data = ky_na)
  pred <- as.numeric(predict(cv_mod, ky_na[folds == i, ], id = i, na.rm = T))
  cv.errors[i] <- mean((ky_na$tmax[folds == i] - pred)^2, na.rm = T)
}
# View mean CV error
# mean(cv.errors)

# Create model if mean CV error is satisfactory
tmax_mod <- glm(tmax ~ tavg + days_into_year, data = ky_na)

# Insert model values of tmax where there is currently and NA
ky_weather$tmax[is.na(ky_weather$tmax)] <- predict(tmax_mod, ky_weather[is.na(ky_weather$tmax), ])
```

### Replace min temp NA values
```{r}
# Create dummy data frame for values that are not NA in the shown variables
ky_na <- ky_weather %>%
  filter(!is.na(tmin) | !is.na(tavg))

set.seed(1212)

# For 10- fold cross validation
k <- 10 

folds <- sample(1:k, nrow(ky_na), replace = TRUE)

# Place holder for errors
cv.errors <- c() 

# Write a for loop that performs cross-validation
for (i in 1:k) {
  dat <- ky_na[folds != i, ]
  cv_mod <- glm(tmin ~ tavg + days_into_year + tmax, data = ky_na)
  pred <- as.numeric(predict(cv_mod, ky_na[folds == i, ], id = i, na.rm = T))
  cv.errors[i] <- mean((ky_na$tmin[folds == i] - pred)^2, na.rm = T)
}
# View CV errors
# mean(cv.errors)

# Create model if mean CV error is satisfactory
tmin_mod <- glm(tmin ~ tavg + days_into_year + tmax, data = ky_na)

# Insert model values of tmax where there is currently and NA
ky_weather$tmin[is.na(ky_weather$tmin)] <- predict(tmin_mod, ky_weather[is.na(ky_weather$tmin), ])
```

### Finish cleaning Kyoto weather data
```{r}
# Now that I have all tmax and min values modeled I can replace tavg values with average with min and max
#   replace missing tavg values with avg of min and max tempuratures of said day
#   this is imperfect as the values are slightly different than the actual average for the day
#   however, I believe this to be more useful than deleting the rows.

ky_weather <- ky_weather %>%
  group_by(year, day, month) %>%
  mutate("avg_T" = (tmax + tmin) / 2)
ky_weather$tavg[is.na(ky_weather$tavg)] <- ky_weather$avg_T[is.na(ky_weather$tavg)]

# Remove unneeded variables
ky_weather <- ky_weather %>%
  select(-avg_T, -snow_mm, -wind_dir_deg, -`wind_sp_km/h`, -`peak_wind_km/`, -pres_hPa, -sun_min)

# Convert celcius to kelvin 
ky_weather$tavg <- ky_weather$tavg + 273.15
ky_weather$tmin <- ky_weather$tmin + 273.15
ky_weather$tmax <- ky_weather$tmax + 273.15

colnames(ky_weather)[4:6] <- c("tavg_K", "tmin_K", "tmax_K")

write_csv(ky_weather,"ky_weather_cleaned.csv")
```

### Get Kyoto blossom tree data
```{r}
# Read in DC cherry tree data
ky_cherry <- read_csv("data/kyoto.csv") %>%
  
  # Remove 2005 because no weather data available for that year
  filter(year != 2005)

```

### Create new data frame with kyoto bloom data and modeling variables
```{r}
## Now that I have replaced missing values I will combine the blooming data set and weather dataset.
# Filter cherry tree data for past 1951 (first year in which all data is there)
ky_cherry_w_weath <- ky_cherry %>%
  filter(year >= 1951)

# Initialize tibble with first row
ky_bloom <- as_tibble(bloom_merge(ky_cherry_w_weath, ky_weather, 1951))

# Loop through all other rows to create data frame with all variables from bloom_merge function
for (i in 2:nrow(ky_cherry_w_weath)) {
  yr <- ky_cherry_w_weath$year[i]
  ky_bloom[i, ] <- bloom_merge(ky_cherry_w_weath, ky_weather, yr)
}

write_csv(ky_bloom,"ky_model_data")
```

## Test old model first

### Now test how accurate predictions from DC model are for Kyoto
```{r}
x_test <- model.matrix(bloom_doy ~ ., ky_bloom[, 7:52])[, -1]

# Make predictions from elastic net regression model from DC data
dc_net_pred_ky <- fit_net_dc %>%
  predict(x_test) %>%
  as.vector()

# View model performance metrics
tibble(
  RMSE = RMSE(dc_net_pred_ky, ky_bloom$bloom_doy),
  Rsquare = R2(dc_net_pred_ky, ky_bloom$bloom_doy)
)
ky_en_ridge_preds <- tibble(ky_bloom$year, ky_bloom$bloom_doy, dc_net_pred_ky)
```

Not as good, I want to see if I can make the prediction better by combining the bloom datasets then running a model over the combined data. 

## Try combining data to make a better model

### Combine rows of dc_bloom & ky_bloom
```{r}
blooms <- rbind(dc_bloom, ky_bloom)
```

### Build training and testing sets
```{r}
# Create training and test sets
training_samples <- blooms$bloom_doy %>%
  createDataPartition(p = 0.75, list = FALSE)
train_data <- blooms[training_samples, c(2, 7:52)]
test_data <- blooms[-training_samples, c(2,7:52)]

# Set up data in matrix to run ridge regression
x <- model.matrix(bloom_doy ~ ., train_data)[, -1]
y <- train_data$bloom_doy

x_test <- model.matrix(bloom_doy ~ ., test_data)[, -1]
```

### Elastic Net regression
```{r}
## Using elastic net regression to try to find best combination of alpha and lambda parameters

# Build the model using the training set
set.seed(1212)
cv_elas_net <- train(
  bloom_doy ~ .,
  data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)

# Best tuning parameter
(en_lambda <- cv_elas_net$bestTune)
```

### Ridge regression with elastic net lambda
```{r}
# Fit elastic net regression model to training data using optimal 
# lambda value found from cross validation
fit_net_dc_ky <- glmnet(x, y, 
                  alpha = cv_elas_net$bestTune[1], 
                  lambda = cv_elas_net$bestTune[2])

# View coefficients
#coef(fit_net_dc_ky)


# Make predictions from new elastic net regression model
net_pred_dc_ky <- fit_net_dc_ky %>%
  predict(x_test) %>%
  as.vector()

# View model performance metrics
tibble(
  RMSE = RMSE(net_pred_dc_ky, test_data$bloom_doy),
  Rsquare = R2(net_pred_dc_ky, test_data$bloom_doy)
)

dcky_net <- tibble(test_data$location, blooms$year[-training_samples], test_data$bloom_doy, net_pred_dc_ky)
```

This is a much better result than anything produced previously. I will next try to add Liestal to the data.

# Rework model adding in Liestal Data

### Get Liestal data from NOAA database
```{r}
## Data base that had data for other 3 cities did not have data from Liestal so I will use te noaa package to get data
liest <- ghcnd_search(
  stationid = "GME00127786", var = c("all"),
  date_min = "1950-01-01", date_max = "2021-12-31"
)

# Get max and min temps
liest_max <- liest[[1]]
liest_min <- liest[[2]]
```

### Get into format to create model variables
```{r}
# Join temps together with date
li_weather <- liest_max %>%
  left_join(liest_min, by = c("date" = "date")) %>%
  
  # Keep only relevant columns
  select(date, tmin, tmax) %>%
  
  # Separate month, day & year
  extract(date,
    into = c("year", "month", "day"),
    regex = "([0-9]+)[-]([0-9]+)[-]([0-9]+)$"
  ) %>%
  
  # Make individual date variables numeric
  mutate(across(c(year, day, month), as.numeric)) %>%
  
  # Add days into year column
  group_by(year) %>%
  mutate("days_into_year" = row_number()) %>%
  ungroup()

#Convert temperatures to Kelvin
li_weather$tmin <- li_weather$tmin/10 + 273.15
li_weather$tmax <- li_weather$tmax/10 + 273.15

colnames(li_weather)[4:5] <- c("tmin_K", "tmax_K")
```

Model to find tavg must be created because simply taking the average of the max and min over the entire data set will create colinearity. 

### To calculate tavg - need to create a model
```{r}
weath_comb <- rbind(dc_weather, ky_weather)

set.seed(1212)

# For 10- fold cross validation
k <- 10 

folds <- sample(1:k, nrow(weath_comb), replace = TRUE)

# Place holder for errors
cv.errors <- c() 

# Write a for loop that performs cross-validation
for (i in 1:k) {
  dat <- weath_comb[folds != i, ]
  cv_mod <- glm(tavg_K ~ tmin_K + days_into_year + tmax_K + year, data = weath_comb)
  pred <- as.numeric(predict(cv_mod, weath_comb[folds == i, ], id = i, na.rm = T))
  cv.errors[i] <- mean((weath_comb$tavg_K[folds == i] - pred)^2, na.rm = T)
}
# View CV error
#mean(cv.errors)

# Run model once satisfied with CV error
tavg_mod <- glm(tavg_K ~ tmin_K + days_into_year + tmax_K + year, data = weath_comb)

# Create average temperature variable and fill it with model predictions
li_weather$tavg_K <- predict(tavg_mod, li_weather)

# Reorder variables in weather data frame
li_weather <- li_weather[, c(1:3, 7, 4:6)]

write_csv(li_weather, "li_weather_cleaned.csv")
```

### Import Liestal blossom data
```{r}
# Read in DC cherry tree data
li_cherry <- read_csv("data/liestal.csv") %>%
  
  # Missing weather data for 2015
  filter(year != 2015)
```

### Create new data frame with liestal  bloom data and modeling variables
```{r}
# Filter cherry tree data for past 1954 (first year in which there is all data)
li_cherry_w_weath <- li_cherry %>%
  filter(year >= 1954)

# Initialize tibble with first row
li_bloom <- as_tibble(bloom_merge(li_cherry_w_weath, li_weather, 1954))

# Loop through all other rows to create data frame with all variables from bloom_merge function
for (i in 2:nrow(li_cherry_w_weath)) {
  yr <- li_cherry_w_weath$year[i]
  li_bloom[i, ] <- bloom_merge(li_cherry_w_weath, li_weather, yr)
}

write_csv(li_bloom,"li_model_data.csv")
```


# Create new model adding Liestal data

### Combine rows of dc_bloom & ky_bloom
```{r}
blooms <- rbind(dc_bloom, ky_bloom, li_bloom)
```

### Build training and testing sets
```{r}
# Create training and test sets
set.seed(1212)
training_samples <- blooms$bloom_doy %>%
  createDataPartition(p = .75, list = FALSE)
train_data <- blooms[training_samples, c(5, 7:17)]
test_data <- blooms[-training_samples, c( 5,7:17)]

# Set up data in matrix to run ridge regression
x <- model.matrix(bloom_doy ~ ., train_data)[, -1]
y <- train_data$bloom_doy

x_test <- model.matrix(bloom_doy ~ ., test_data)[, -1]
```

### Elastic Net regression
```{r}
## Using elastic net regression to try to find best combination of alpha and lambda parameters

# Build the model using the training set
set.seed(1212)
cv_elas_net <- train(
  bloom_doy ~ .,
  data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 100
)

# Best tuning parameter
(en_lambda <- cv_elas_net$bestTune)
```

### elastic net regression for the 3 cities
```{r}
# Fit elastic net regression model to training data using optimal lambda value found
# from cross validation
fit_net_dc_ky_li <-glmnet(x, y, alpha = cv_elas_net$bestTune[1], lambda = cv_elas_net$bestTune[2])

# View coefficients
coef(fit_net_dc_ky_li)
#summary(fit_net_dc_ky_li)

# Make predictions from net elastic regression model
net_pred_dc_ky_li <- fit_net_dc_ky_li %>%
  predict(x_test) %>%
  as.vector()

# View model performance metrics
tibble(
  net_RMSE = RMSE(net_pred_dc_ky_li, test_data$bloom_doy),
  net_Rsquare = R2(net_pred_dc_ky_li, test_data$bloom_doy)
)

dckyli_net <- tibble(test_data$location, blooms$year[-training_samples], test_data$bloom_doy, net_pred_dc_ky_li)
```

The model that includes Liestal has a higher root mean squared error than the previous iteration of the model but is still a very acceptable value. 

# Check prediction values on Vancouver weather data

### Import Vancouver Weather and clean
```{r}
van_weather <- read_csv("data/vancuv_weather.csv")

# set column names
weather_names <- c("date", "tavg", "tmin", "tmax", "precip_mm", "snow_mm", "wind_dir_deg", "wind_sp_km/h", "peak_wind_km/", "pres_hPa", "sun_min")
colnames(van_weather) <- weather_names

# Separate month, day & year
van_weather <- van_weather %>%
  extract(date,
    into = c("year", "month", "day"),
    regex = "([0-9]+)[-]([0-9]+)[-]([0-9]+)$"
  ) %>%
  
  # Make individual date variables numeric
  mutate(across(c(year, day, month), as.numeric)) %>%
  
  # Start of full year weather data being captured
  filter(year >= 1958) %>%
  
  # add total days count column by year
  group_by(year) %>%
  mutate("days_into_year" = row_number()) %>%
  ungroup()
```

### Replace 1 missing tavg value with avg of values around it
```{r}
ind <- which(is.na(van_weather$tavg))
van_avg <- mean(van_weather$tavg[c((ind - 5):(ind - 1), (ind + 1):(ind + 5))])
van_weather$tavg[is.na(van_weather$tavg)] <- van_avg
```


### Create model to replace missing tmin values
```{r}
van_na <- van_weather %>%
  filter(!is.na(tmax) | !is.na(tmin))

set.seed(1212)

# 10- fold cross validation
k <- 10 

folds <- sample(1:k, nrow(van_na), replace = TRUE)

# Place holder for errors
cv.errors <- c() 

# write a for loop that performs cross-validation
for (i in 1:k) {
  dat <- van_na[folds != i, ]
  cv_mod <- glm(tmin ~ tavg + days_into_year, data = van_na)
  pred <- as.numeric(predict(cv_mod, van_na[folds == i, ], id = i, na.rm = T))
  cv.errors[i] <- mean((van_na$tmin[folds == i] - pred)^2, na.rm = T)
}

# View CV error
#mean(cv.errors)

# Create model if CV error is satisfactory
tmin_mod <- glm(tmin ~ tavg + days_into_year, data = van_na)

# Update NA values for tmin with model values
van_weather$tmin[is.na(van_weather$tmin)] <- predict(tmin_mod, van_weather[is.na(van_weather$tmin), ])
```

### Create model to replace missing tmin values
```{r}
van_na <- van_weather %>%
  filter(!is.na(tmax))

set.seed(1212)

# 10- fold cross validation
k <- 10

folds <- sample(1:k, nrow(van_na), replace = TRUE)

# Place holder for errors
cv.errors <- c() 

# write a for loop that performs cross-validation
for (i in 1:k) {
  dat <- van_na[folds != i, ]
  cv_mod <- glm(tmax ~ tmin + tavg + days_into_year, data = van_na)
  pred <- as.numeric(predict(cv_mod, van_na[folds == i, ], id = i, na.rm = T))
  cv.errors[i] <- mean((van_na$tmax[folds == i] - pred)^2, na.rm = T)
}
# View CV error
#mean(cv.errors)

# Create model if CV error is satisfactory
tmax_mod <- glm(tmax ~ tmin + tavg + days_into_year, data = van_na)

# Update NA values for tmax with model values
van_weather$tmax[is.na(van_weather$tmax)] <- predict(tmax_mod, van_weather[is.na(van_weather$tmax), ])
```

### Complete cleaning of vancouver weather data
```{r}
# Remove unneeded variables 
van_weather <- van_weather %>%
  select(-snow_mm, -wind_dir_deg, -`wind_sp_km/h`, -`peak_wind_km/`, -pres_hPa, -sun_min)

# Convert celcius to kelvin 
van_weather$tavg <- van_weather$tavg + 273.15
van_weather$tmin <- van_weather$tmin + 273.15
van_weather$tmax <- van_weather$tmax + 273.15


colnames(van_weather)[4:6] <- c("tavg_K", "tmin_K", "tmax_K")

write_csv(van_weather,"vc_weather_cleaned.csv")
```

### Create dummy df for vc bloom data
```{r}
set.seed(1212)
year <- c(1978:2004, 2006:2021)
rand_doy <- sample(91:110, length(year), replace = T)

vc_cherry <- tibble(
  "location" = "vancouver",
  "lat" = 49.2237,
  "long" = -123.1636,
  "alt" = 24,
  "year" = year,
  "bloom_date" = "2020-03-29",
  "bloom_doy" = rand_doy
)
```

### Create data frame with variables for modeling
```{r}
vc_cherry_w_weath <- vc_cherry %>%
  filter(year >= 1978)

# Initialize tibble with first row
vc_bloom <- as_tibble(bloom_merge(vc_cherry_w_weath, van_weather, 1978))
# Loop through all other rows to create data frame with all variables from bloom_merge function
for (i in 2:nrow(vc_cherry_w_weath)) {
  yr <- vc_cherry_w_weath$year[i]
  vc_bloom[i, ] <- bloom_merge(vc_cherry_w_weath, van_weather, yr)
}

write_csv(vc_bloom,"vc_model_data.csv")
```

# Test model with Vancouver data

### Combine vancouver modeling dataset with test data from last model
```{r}
all_loc = rbind(test_data,vc_bloom[,c(2,7:12)])
x_test <- model.matrix(bloom_doy ~ ., all_loc)[, -1]
```

In this next part I am not looking for accuracy of the model but just make sure the predictions make sense. Because I do not have actual DOY for blooms in Vancouver, I can't find the actual accuracy. 

### Run model from dc and kyoto to see if predictions make sense.
```{r}
net_pred_dc_ky_li_vc <- fit_net_dc_ky_li %>%
  predict(x_test) %>%
  as.vector()

# Note: vancouver values are rows 53 and on
dckylivc_net <- tibble( all_loc$year, all_loc$bloom_doy, net_pred_dc_ky_li_vc)


```


The numbers for the vancouver values seem in line with where they should be so I will now just rename the model for use in predicting next 10 years. 

### Rename model
```{r}
next_10_years_model = fit_net_dc_ky_li
coef(next_10_years_model)
```



# Plot linear correlation
```{r}
ggplot(data = blooms) +
  geom_point(aes(y = bloom_doy, x = third_30_maxT), color = "darkblue", alpha = .8) +
  labs(title = "Example of Variable Correlation",
       x = "Cumulative Maximum Temperature for days 60-90",
       y = "Bloom Day of the Year") +
  
   theme(
    plot.title = element_text(
      size = 25, # Enlarge & center title
      margin = margin(10, 0, 10, 0), hjust = .5
    ),
    axis.title = element_text(size = 16),
    axis.text = element_text(size = 14),
    panel.grid.minor = element_line(color = "#D3D3D3", size = 0.2),
    panel.grid.major = element_blank(), # Remove major grid
    panel.background = element_blank()
  ) 
ggsave("figure1.png", width = 15, height = 12, dpi = "retina")


```




Now that I have my model for predicting the bloom day, I need to have a model that predicts the temperatures of the next 10 years in each location and then predict the dates of bloom for the next 10 years. 

# Make model for predicting temperatures for next 10 years

### Add columns to all weather datasets then combine for modeling
```{r}
dc_weath_comb = dc_weather
dc_weath_comb$location = "washingtondc"
dc_weath_comb$lat = 38.8853
dc_weath_comb$long = -77.0386
dc_weath_comb$alt = 0
dc_weath_comb$rand = rnorm(nrow(dc_weath_comb), mean = 287.5, sd = 9.5)

ky_weath_comb = ky_weather
ky_weath_comb$location = "kyoto"
ky_weath_comb$lat = 35.0120
ky_weath_comb$long = 135.6761
ky_weath_comb$alt = 44
ky_weath_comb$rand = rnorm(nrow(ky_weath_comb), mean = 289, sd = 8.7)

li_weath_comb = li_weather
li_weath_comb$location = "liestal"
li_weath_comb$lat = 47.4814
li_weath_comb$long = 7.730519
li_weath_comb$alt = 350
li_weath_comb$rand = rnorm(nrow(li_weath_comb), mean = 284, sd = 7.5)

vc_weath_comb = van_weather
vc_weath_comb$location = "kyoto"
vc_weath_comb$lat = 49.2237
vc_weath_comb$long = -123.1636
vc_weath_comb$alt = 24
vc_weath_comb$rand = rnorm(nrow(vc_weath_comb), mean = 283.5, sd = 5.8)



weath_comb = rbind(dc_weath_comb,
                   ky_weath_comb,
                   li_weath_comb,
                   vc_weath_comb)
```

### Do cross validation for model 
```{r}
library(gam)

set.seed(1212)
weath_comb = weath_comb %>%
  filter(days_into_year <= 150)
# 10- fold cross validation
k <- 10

folds <- sample(1:k, nrow(weath_comb), replace = TRUE)

# Place holder for errors
cv.errors <- c()

# write a for loop that performs cross-validation
for (i in 1:k) {
  dat <- weath_comb[folds != i, ]
  cv_mod <- lm(tavg_K ~ s(year)+
                   poly(days_into_year,2)+
                   lat
                 , data = weath_comb[folds != i, ])
  
  pred <- as.numeric(predict(cv_mod, weath_comb[folds == i, ], id = i, na.rm = T))
  cv.errors[i] <- mean((weath_comb$tavg_K[folds == i] - pred)^2, na.rm = T)
}
# View CV error
mean(cv.errors)
```


### Create Models
```{r}
# Create model if CV error is satisfactory
tavg_pred_model <- lm(tavg_K ~ s(year)+
                   poly(days_into_year,2)+
                   lat
                 , data = weath_comb)
tmax_pred_model <- lm(tmax_K ~ s(year)+
                   poly(days_into_year,2)+
                   lat
                 , data = weath_comb)
tmin_pred_model <- lm(tmin_K ~ s(year)+
                   poly(days_into_year,2)+
                   lat
                 , data = weath_comb)

```

```{r}
summary(tavg_pred_model)

```
# Make Prediction for DC

### Create column of dates
```{r}
dates = tibble("date" = seq(as.Date("2022-01-01"),as.Date("2031-12-31"),"days")) %>%
  extract(date,
    into = c("year", "month", "day"),
    regex = "([0-9]+)[-]([0-9]+)[-]([0-9]+)$",
    remove = FALSE) %>%
  # Make individual date variables numeric
  mutate(across(c(year, day, month), as.numeric)) %>%
  
  # add total days count column by year
  group_by(year) %>%
  mutate("days_into_year" = row_number()) %>%
  ungroup() %>%
  filter(days_into_year <= 150)

```

### Create dataframe for DC predictions
```{r}
dc_weather_pred = dates[2:5]
dc_weather_pred$location = "washingtondc"
dc_weather_pred$lat = 38.8853
dc_weather_pred$long = -77.0386
dc_weather_pred$alt = 0
dc_weather_pred$rand = rnorm(nrow(dc_weather_pred), mean = 287.5, sd = 9.5)

dc_weather_pred$tavg_K = predict(tavg_pred_model,dc_weather_pred)
dc_weather_pred$tmax_K = predict(tmax_pred_model,dc_weather_pred)
dc_weather_pred$tmin_K = predict(tmin_pred_model,dc_weather_pred)

```


```{r}
samp = function(len, probs, weather,day) {
  s = sample(1:len,1,prob = probs)
  wet = weather %>%
      filter(days_into_year == day)
  vec = wet[rand,4:6]
  if(any(is.na(vec))){
    vec = samp(len,probs,weather)
  }
  return(vec)
}

predict_weather = function(weather,weather_pred){

  # Amount of years in dataset
  len = length(unique(weather$year))

  # Probability weather will be chosen outside of last 20 years
  left_over_prob = .3/(len-20)

  # Probabilities that weather is chosen from given years
  probs = c(rep(left_over_prob,len-20),rep(.020,10),rep(.05,10))

  # Loop through each day & choose weather randomly from that day of year
  #   based on weighted probabilities - more recent years, higher probability.
  for(i in 1:nrow(weather_pred)){

    # Select day of year
    d = weather_pred$days_into_year[i]

    # Get index to get weather from
    #rand = sample(1:len,1,prob = probs)

    # Filter for Day of year
    #wet = weather %>%
      #filter(days_into_year == d)

    # Extract weather
    #vec = wet[rand,4:6]
    vec = samp(len,probs,weather,day)

    # Save to predicted weather df
    weather_pred[i,9:11] = vec
  }

  # Put into correct format
  weather_pred = weather_pred[,c(1:3,9:11,4)]
  return(weather_pred)

}
```

```{r}
dc_weather_pred = dates[2:5]
dc_weather_pred$location = "washingtondc"
dc_weather_pred$lat = 38.8853
dc_weather_pred$long = -77.0386
dc_weather_pred$alt = 0
dc_weather_pred$tavg_K = 1
dc_weather_pred$tmax_K = 1
dc_weather_pred$tmin_K = 1

set.seed(1)

dc_weather_pred = predict_weather(dc_weather, dc_weather_pred)
```

### Replace already known weather data for 2022
```{r}
known_dc_2022 = read_csv("data/DC_weather.csv") 

# set column names
weather_names <- c("date", "tavg_K", "tmin_K", "tmax_K", "precip_mm", "snow_mm", "wind_dir_deg", "wind_sp_km/h", "peak_wind_km/", "pres_hPa", "sun_min")
colnames(known_dc_2022) <- weather_names
  
known_dc_2022 = known_dc_2022 %>%
  extract(date,
    into = c("year", "month", "day"),
    regex = "([0-9]+)[-]([0-9]+)[-]([0-9]+)$") %>%
  # Make individual date variables numeric
  mutate(across(c(year, day, month), as.numeric)) %>%

  filter(year == 2022)

known_dc_2022$tavg_K = known_dc_2022$tavg_K + 273.15
known_dc_2022$tmin_K = known_dc_2022$tmin_K + 273.15
known_dc_2022$tmax_K = known_dc_2022$tmax_K + 273.15


dc_weather_pred$tavg_K[1:nrow(known_dc_2022)] = known_dc_2022$tavg_K
dc_weather_pred$tavg_K[1:nrow(known_dc_2022)] = known_dc_2022$tavg_K
dc_weather_pred$tavg_K[1:nrow(known_dc_2022)] = known_dc_2022$tavg_K

```

### Get into format for predicting 
```{r}
set.seed(1212)
dc_bloom_pred = tibble("location" = "washingtondc",
                       "lat" = 38.8853,
                       "long" = -77.0386,
                       "alt" = 0,
                       "year" = 2022:2031,
                       "bloom_date" = "2022-01-01",
                       "bloom_doy" = sample(80:120,10,replace = TRUE)
                       )
dc_weather_pred = dc_weather_pred[,c(1:3,10:12,4)]

```

### Create Variables for model input
```{r}
# Initialize tibble with first row
dc_bloom_predictors <- as_tibble(bloom_merge(dc_bloom_pred, dc_weather_pred, 2022))

# Loop through all other rows to create data frame with all variables from bloom_merge function
for (i in 2:nrow(dc_bloom_pred)) {
  yr <- dc_bloom_pred$year[i]
  dc_bloom_predictors[i, ] <- bloom_merge(dc_bloom_pred, dc_weather_pred, yr)
}
```

### Run model for DC
```{r}
x_test <- model.matrix(bloom_doy ~ ., dc_bloom_predictors[,c( 5,7:17)])[, -1]

dc_fut_pred <- next_10_years_model %>%
  predict(x_test) %>%
  as.vector()

(dc_predictions = tibble( "year" = dc_bloom_predictors$year, 
                         "prediction" = round(dc_fut_pred,0)))


```

### Create Data Frame for all predictions
```{r}
final_predictions = tibble("year" = dc_predictions$year,
                           "kyoto" = 1,
                           "liestal" = 1,
                           "washingtondc" = dc_predictions$prediction,
                           "vancouver" = 1
                           )
```

# Make Predictions for Kyoto

### Repeat for Kyoto
```{r}
ky_weather_pred = dates[2:5]
ky_weather_pred$location = "kyoto"
ky_weather_pred$lat = 35.0120	
ky_weather_pred$long = 135.6761
ky_weather_pred$alt = 44
ky_weather_pred$rand = rnorm(nrow(ky_weather_pred), mean = 289, sd = 8.7)

ky_weather_pred$tavg_K = predict(tavg_pred_model,ky_weather_pred)
ky_weather_pred$tmax_K = predict(tmax_pred_model,ky_weather_pred)
ky_weather_pred$tmin_K = predict(tmin_pred_model,ky_weather_pred)

```

### Replace already known weather data for 2022
```{r}
known_ky_2022 = read_csv("data/kyoto_weather.csv") 

# set column names
weather_names <- c("date", "tavg_K", "tmin_K", "tmax_K", "precip_mm", "snow_mm", "wind_dir_deg", "wind_sp_km/h", "peak_wind_km/", "pres_hPa", "sun_min")
colnames(known_ky_2022) <- weather_names
  
known_ky_2022 = known_ky_2022 %>%
  extract(date,
    into = c("year", "month", "day"),
    regex = "([0-9]+)[-]([0-9]+)[-]([0-9]+)$") %>%
  # Make individual date variables numeric
  mutate(across(c(year, day, month), as.numeric)) %>%

  filter(year == 2022)

known_ky_2022$tavg_K = known_ky_2022$tavg_K + 273.15
known_ky_2022$tmin_K = known_ky_2022$tmin_K + 273.15
known_ky_2022$tmax_K = known_ky_2022$tmax_K + 273.15


ky_weather_pred$tavg_K[1:nrow(known_ky_2022)] = known_ky_2022$tavg_K
ky_weather_pred$tavg_K[1:nrow(known_ky_2022)] = known_ky_2022$tavg_K
ky_weather_pred$tavg_K[1:nrow(known_ky_2022)] = known_ky_2022$tavg_K

```

### Get into format for predicting 
```{r}
set.seed(1212)
ky_bloom_pred = tibble("location" = "kyoto",
                       "lat" = 35.0120,
                       "long" = 135.6761,
                       "alt" = 44,
                       "year" = 2022:2031,
                       "bloom_date" = "2022-01-01",
                       "bloom_doy" = sample(80:120,10,replace = TRUE)
                       )
ky_weather_pred = ky_weather_pred[,c(1:3,10:12,4)]

```

### Create Variables for model input
```{r}
# Initialize tibble with first row
ky_bloom_predictors <- as_tibble(bloom_merge(ky_bloom_pred, ky_weather_pred, 2022))

# Loop through all other rows to create data frame with all variables from bloom_merge function
for (i in 2:nrow(ky_bloom_pred)) {
  yr <- ky_bloom_pred$year[i]
  ky_bloom_predictors[i, ] <- bloom_merge(ky_bloom_pred, ky_weather_pred, yr)
}
```

### Run model
```{r}
x_test <- model.matrix(bloom_doy ~ ., ky_bloom_predictors[,c(5, 7:16)])[, -1]


ky_fut_pred <- next_10_years_model %>%
  predict(x_test) %>%
  as.vector()

(ky_predictions = tibble( "year" = ky_bloom_predictors$year, 
                         "prediction" = round(ky_fut_pred,0)))

```
### Update final predictions
```{r}
final_predictions$kyoto = ky_predictions$prediction

```

# Make predictions for Vancouver

### Repeat for Vancouver
```{r}
vc_weather_pred = dates[2:5]
vc_weather_pred$location = "vancouver"
vc_weather_pred$lat = 49.2237	
vc_weather_pred$long = -123.1636
vc_weather_pred$alt = 24
vc_weather_pred$rand = rnorm(nrow(vc_weather_pred), mean = 283.5, sd = 5.8)

vc_weather_pred$tavg_K = predict(tavg_pred_model,vc_weather_pred)
vc_weather_pred$tmax_K = predict(tmax_pred_model,vc_weather_pred)
vc_weather_pred$tmin_K = predict(tmin_pred_model,vc_weather_pred)

```

### Replace already known weather data for 2022
```{r}
known_vc_2022 = read_csv("data/vancuv_weather.csv") 

# set column names
weather_names <- c("date", "tavg_K", "tmin_K", "tmax_K", "precip_mm", "snow_mm", "wind_dir_deg", "wind_sp_km/h", "peak_wind_km/", "pres_hPa", "sun_min")
colnames(known_vc_2022) <- weather_names
  
known_vc_2022 = known_vc_2022 %>%
  extract(date,
    into = c("year", "month", "day"),
    regex = "([0-9]+)[-]([0-9]+)[-]([0-9]+)$") %>%
  # Make individual date variables numeric
  mutate(across(c(year, day, month), as.numeric)) %>%

  filter(year == 2022)

known_vc_2022$tavg_K = known_vc_2022$tavg_K + 273.15
known_vc_2022$tmin_K = known_vc_2022$tmin_K + 273.15
known_vc_2022$tmax_K = known_vc_2022$tmax_K + 273.15


vc_weather_pred$tavg_K[1:nrow(known_vc_2022)] = known_vc_2022$tavg_K
vc_weather_pred$tavg_K[1:nrow(known_vc_2022)] = known_vc_2022$tavg_K
vc_weather_pred$tavg_K[1:nrow(known_vc_2022)] = known_vc_2022$tavg_K

```

### Get into format for predicting 
```{r}
set.seed(1212)
vc_bloom_pred = tibble("location" = "vancouver",
                       "lat" = 49.2237,
                       "long" = -123.1636,
                       "alt" = 24,
                       "year" = 2022:2031,
                       "bloom_date" = "2022-01-01",
                       "bloom_doy" = sample(80:120,10,replace = TRUE)
                       )
vc_weather_pred = vc_weather_pred[,c(1:3,10:12,4)]

```

### Create Variables for model input
```{r}
# Initialize tibble with first row
vc_bloom_predictors <- as_tibble(bloom_merge(vc_bloom_pred, vc_weather_pred, 2022))

# Loop through all other rows to create data frame with all variables from bloom_merge function
for (i in 2:nrow(vc_bloom_pred)) {
  yr <- vc_bloom_pred$year[i]
  vc_bloom_predictors[i, ] <- bloom_merge(vc_bloom_pred, vc_weather_pred, yr)
}
```

### Run Model
```{r}
x_test <- model.matrix(bloom_doy ~ ., vc_bloom_predictors[,c(5, 7:16)])[, -1]


vc_fut_pred <- next_10_years_model %>%
  predict(x_test) %>%
  as.vector()

(vc_predictions = tibble( "year" = vc_bloom_predictors$year, 
                         "prediction" = round(vc_fut_pred,0)))

```
### Update final predictors
```{r}
final_predictions$vancouver = vc_predictions$prediction

```

# Make predictions for Liestal

### Repeat for Liestal
```{r}
li_weather_pred = dates[2:5]
li_weather_pred$location = "liestal"
li_weather_pred$lat = 47.4814	
li_weather_pred$long = 7.730519
li_weather_pred$alt = 350
li_weather_pred$rand = rnorm(nrow(li_weather_pred), mean = 284, sd = 7.5)

li_weather_pred$tavg_K = predict(tavg_pred_model,li_weather_pred)
li_weather_pred$tmax_K = predict(tmax_pred_model,li_weather_pred)
li_weather_pred$tmin_K = predict(tmin_pred_model,li_weather_pred)

```


### Get into format for predicting 
```{r}
set.seed(1212)
li_bloom_pred = tibble("location" = "liestal",
                       "lat" = 47.4814,
                       "long" = 7.730519,
                       "alt" = 350,
                       "year" = 2022:2031,
                       "bloom_date" = "2022-01-01",
                       "bloom_doy" = sample(80:120,10,replace = TRUE)
                       )
li_weather_pred = li_weather_pred[,c(1:3,10:12,4)]

```

### Create Variables for model input
```{r}
# Initialize tibble with first row
li_bloom_predictors <- as_tibble(bloom_merge(li_bloom_pred, li_weather_pred, 2022))

# Loop through all other rows to create data frame with all variables from bloom_merge function
for (i in 2:nrow(li_bloom_pred)) {
  yr <- li_bloom_pred$year[i]
  li_bloom_predictors[i, ] <- bloom_merge(li_bloom_pred, li_weather_pred, yr)
}
```

### Run model
```{r}
x_test <- model.matrix(bloom_doy ~ ., li_bloom_predictors[,c(5, 7:16)])[, -1]


li_fut_pred <- next_10_years_model %>%
  predict(x_test) %>%
  as.vector()

(li_predictions = tibble( "year" = li_bloom_predictors$year, 
                         "prediction" = round(li_fut_pred,0)))

```
### Update predictions
```{r}
final_predictions$liestal = li_predictions$prediction

```


# Save predictions as a .csv
```{r}
write_csv(final_predictions, file = "cherry-predictions.csv")

```